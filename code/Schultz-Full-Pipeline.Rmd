---
title: "Zooplankton COI Metabarcoding Taxonomic ID Pipeline"
author: "Haila Schultz"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,         # Display code chunks
  eval = FALSE,        # Evaluate code chunks
  warning = FALSE,     # Hide warnings
  message = FALSE,     # Hide messages
  fig.width = 6,       # Set plot width in inches
  fig.height = 4,      # Set plot height in inches
  fig.align = "center" # Align plots to the center
)
```
# End Goal
My end goal is to make several figures visualizing the metabarcoding data. One of them will look like the plot below (not actually my data). It will be a series of maps from different months and years showing pie charts of the top taxa identified in my samples. I am currently working on the code to do this mapping. 
![End Goal Example](/Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/screenshots/end_goal_example.png)


# Project Summary

The report is located here: https://rpubs.com/HailaSchultz/full-pipeline

This project utilizes metabarcoding and the LrCOI marker to taxonomically identify zooplankton samples from Washington Ocean Acidification Center Cruises. Samples were collected from seven stations spread throughout Puget Sound in April, July, and September 2018-2020. Zooplankton samples were collected using a 200 um net towed vertically from 10 meters from the bottom of the water column to the surface. A 5 ml subsample (settled volume) of each sample was homogenized, dried, and subsampled before being sent off to Ohio State University for library prep and sequencing.

![Map of the seven sites visited](/Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/screenshots/woac_map.png)

This workflow is adapted from the following pipeline: https://benjjneb.github.io/dada2/ITS_workflow.html

# Download Data
This is step 1 of my project workflow. In this step, I am organizing my fastq files and checking their hash values. I downloaded my data from a shared google drive folder onto my computer and then uploaded into RStudio. Originally, the fastq files were located in multiple layers of directories.

Install dada2 package using BiocManager 
```{r}
install.packages('ShortRead')
install.packages("BiocManager")
BiocManager::install("dada2")
```

load packages
```{r, eval=TRUE}
library(knitr)
library(BiocManager)
library(ShortRead)
library("dada2")
library(dplyr)
library(janitor)
library(vegan)
library(data.table)
library(ggplot2)
library(ggpubr)
library(dplyr)
library(tidyverse)
library(DT)
```


## Move files into one folder

move multiple files from multiple layers of directories into the data directory
```{r, engine="bash"}
cd ../data
find . -name '*.gz' -exec mv {} ../data/fastq-files/ \;
```


create checksums file for all fastq files in data folder
```{r, engine="bash", eval=TRUE, output.lines=4}
cd /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/data/
shasum ../data/fastq-files/*.gz > fastq_checksums.sha
cat fastq_checksums.sha
```

check if files match original
```{r, engine="bash", eval=TRUE, output.lines=4}
cd ../data
shasum -c fastq_checksums.sha
```



# Remove Primers with Cutadapt

To get the cutadapt software, I downloaded the miniconda installer using curl 
```{r, engine="bash"}
#download miniconda from url
cd ../software
curl -O https://repo.anaconda.com/miniconda/Miniconda3-py310_23.3.1-0-MacOSX-arm64.sh

```

I then moved to the directory it was in by typing `cd software` in the terminal. I then typed bash Miniconda3-py310_23.3.1-0-MacOSX-arm64.sh in the terminal to start the setup process. Conda is located in: /Users/hailaschultz/miniconda3. 

I checked that installation worked by typing `conda list` in the terminal. In the terminal I then typed:
`conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
conda config --set channel_priority strict`

and then `conda create -n cutadaptenv cutadapt`

On my personal computer, I had to use `CONDA_SUBDIR=osx-64 conda create -n cutadaptenv cutadapt`

I ran `conda init bash`

To activate the conda environment, in the terminal I typed `conda activate cutadaptenv`

## Prepare data

First, I wanted to check the quality of the raw data to compare with future steps in the pipeline.
download fastqc
```{r, engine="bash"}
curl -O https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.7.zip
```

use fastqc 
```{r, engine="bash"}
cd /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/data/fastq-files
/Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/software/FastQC/fastqc *fastq.gz -o /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/output/raw-read-qc
```

make multiqc report
in the terminal I ran `conda install multiqc` to install multiqc

```{r, engine="bash"}
cd /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/output/raw-read-qc

multiqc .
```

![screenshot of multiqc mean qualit scores](/Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/screenshots/raw-data_multiqc.png)

I then mad lists of the file names in order to process multiple files at once. I also specified the primer sequences used in this prject.
```{r, eval=TRUE, cache=TRUE}
#set file path
path <- "../data/fastq-files"
#make matched list of forward and reverse reads
fnFs <- sort(list.files(path, pattern = "_R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2_001.fastq.gz", full.names = TRUE))

#specify primer sequences
FWD <- "GGWACWGGWTGAACWGTWTAYCCYCC"  
REV <- "TANACYTCNGGRTGNCCRAARAAYCA"  
#in the reverse primer, I replaced I with N
```


get orientations of primers
```{r, eval=TRUE, cache=TRUE}
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
```
I then checked if the primers were actually present in my files.
The followincg code counts the number of times primers occur in the first sample.
```{r, eval=TRUE, cache=TRUE}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs[[1]]))
```
It looks like the forward and reverse primers are present in the forward and reverse reads.

## Use Cutadapt
Tell R the path to the cutadapt software
```{r, eval=TRUE, cache=TRUE}
cutadapt <- "/Users/hailaschultz/miniconda3/envs/cutadaptenv/bin/cutadapt" 
#verify that r knows where to find teh software by asking for the version
system2(cutadapt, args = "--version") 
```
Tell cutadapt what to cut
```{r, eval=TRUE, cache=TRUE}
path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC)
```

Run Cutadapt
```{r}
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs[i], fnRs[i])) # input files
}
```

Check if adapters were actuall trimmed
```{r, eval=TRUE, cache=TRUE}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))

```
There were a few reverse reads that were not trimmed, but overall it looks good
However, after trimming the primers, about half of the files ended up with zero data/reads. I think it is an issue with the read names. The error I get is: ERROR: Error in sequence file at unknown line: Reads are improperly paired. Read name 'M02815:67:000000000-KDDDG:1:1101:17448:1214 1:N:0:CGGAGCCT+GTAAGGAG' in file 1 does not match 'M02815:67:000000000-KDDDG:1:1101:20493:1232 2:N:0:CGTACTAG+CGTCTAAT' in file 2, etc.


Delete files that didn't make it through cutadapt - smaller than 1 MB in the cutadapt directory
```{r, engine="bash"}
cd /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/data/fastq-files/cutadapt
find . -type f -name "*.gz" -size -1M -delete
```

## QC check on files after primers were trimmed

use fastqc 
```{r, engine="bash"}
cd /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/data/fastq-files/cutadapt
/Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/software/FastQC/fastqc *fastq.gz -o /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/output/after-cutadapt-qc
```

make multiqc report to check files after trimming primers
```{r, engine="bash"}
cd /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/output/after-cutadapt-qc

multiqc .
```
![screenshot of multiqc mean quality scores after cutadapt](/Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/screenshots/after-cutadapt_multiqc.png)

# Filter Reads

## Inspect read quality
Get the sample names for the files that have been trimmed
```{r, eval=TRUE}
# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "_R1_001.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "_R2_001.fastq.gz", full.names = TRUE))

# Extract sample names, assuming filenames have format:
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)
```

plot forward read quality for first two files
```{r, eval=TRUE}
plotQualityProfile(cutFs[1:2])
```

plot reverse read quality for first two files
```{r, eval=TRUE}
plotQualityProfile(cutRs[1:2])
```
The forward reads are better quality, as we might expect.


Designate file names location for the filtered files
```{r, eval=TRUE}
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))
```

Before filtering, verify that all of your files have a matching pair. Both the forward and reverse reads for each sample are required for this step.
```{r, eval=TRUE}
# get sample names to see which files match
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.namesF <- unname(sapply(cutFs, get.sample.name))
sample.namesR <- unname(sapply(cutRs, get.sample.name))
```

Check for differences between lists and remove files that don't have a matching pair
```{r, eval=TRUE}
difsR <- setdiff(sample.namesR,sample.namesF)
difsR
difsF <- setdiff(sample.namesF,sample.namesR)
difsF

```
It looks like there are no differences, so we can proceed


## filter reads using quality thresholds
maxN: allows zero Ns
max EE: set to 5 for forward and reverse
truncQ: set to 2
minlen: minimum length is 100 reads
rm.phix:remove phix
Note that this step takes a long time
```{r}
out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(5, 5), 
    truncQ = 2, minLen = 100, rm.phix = TRUE, compress = TRUE, multithread = TRUE) 
```

Check how many samples went through filtering
```{bash, eval=TRUE}
cd /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/data/fastq-files/cutadapt

ls | wc -l

cd /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/data/fastq-files/cutadapt/filtered

ls | wc -l
```

learn the error rates

forward read error rates
```{r, eval=TRUE, cache=TRUE}
errF <- learnErrors(filtFs, multithread=TRUE)
```
reverse read error rates
```{r, eval=TRUE, cache=TRUE}
errR <- learnErrors(filtRs, multithread=TRUE)
```

plot the errors
```{r, eval=TRUE}
plotErrors(errF, nominalQ=TRUE)
#looks okay! good to proceed
```
```{r, eval=TRUE}
plotErrors(errR, nominalQ=TRUE)
#looks okay! good to proceed
```

## QC check on files after filtering were trimmed

use fastqc 
```{r, engine="bash"}
cd /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/data/fastq-files/cutadapt/filtered
/Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/software/FastQC/fastqc *fastq.gz -o /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/output/after-filtering-qc
```

make multiqc report to check files after trimming primers
```{r, engine="bash"}
cd /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/output/after-filtering-qc

multiqc .
```

![screenshot of multiqc mean quality scores after filtering](/Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/screenshots/after-filtering_multiqc.png)


# Pair Forward and Reverse Reads

## Dereplication

This step takes a little while
```{r, eval=TRUE}
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```

Apply core sample inference algorithm
```{r, eval=TRUE}
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
```

```{r, eval=TRUE}
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
```

##nMerge Paired Reads
```{r, eval=TRUE, cache=TRUE}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```

## Construct sequence table
```{r, eval=TRUE}
seqtab <- makeSequenceTable(mergers)
```

inspect sequence length distributions
eventually make histogram here
```{r, eval=TRUE}
table(nchar(getSequences(seqtab)))
```

## remove chimeras
```{r, eval=TRUE}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
```

evaluate percentage of reads were chimeric
```{r, eval=TRUE}
sum(seqtab.nochim)/sum(seqtab)
```
chimeras account for very low perentage of reads


track how many reads made it through the pipeline
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```
no step had a majority of the reads removed
The step that creates the out table takes too long to run, so a screenshot of this table is included below

![screenshot of tracking table](/Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/screenshots/Tracking_Table.png)

# Blast

## download software from NCBI
```{r, engine="bash"}
#change directory
cd /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/software
#download software
curl -O https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.14.0+-x64-macosx.tar.gz
```

unzip software
```{r, engine="bash"}
cd /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/software
tar -xzf ncbi-blast-2.14.0+-x64-macosx.tar.gz
```

Check if it's working
I had to go into my computer settings and give permissions to use blast because it is from and "unidentified developer"
```{r, engine="bash", eval=TRUE}
/Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/software/ncbi-blast-2.14.0+/bin/blastx -h
```

## Make blast database

Download the reference. The reference used here is from metazoogene and is the file for all marine fauna and flora of the world oceans
```{r, engine="bash"}
#download from url
cd /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/data
curl -O https://www.st.nmfs.noaa.gov/copepod/collaboration/metazoogene/atlas/data-src/MZGfasta-coi__MZGdbALL__o07__B.fasta
#unzip
gunzip -k MZGfasta-coi__MZGdbALL__o00__A.fasta
```

Make the database
Since we are evaluating nucleotide seqeunces, make sure you are using -dbtype nucl
```{r, engine="bash"}
/Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/software/ncbi-blast-2.14.0+/bin/makeblastdb \
-in /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/data/MZGfasta-coi__MZGdbALL__o00__A.fasta \
-dbtype nucl \
-out /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/blastdb/MZGfasta-coi__MZGdbALL__o00__A
```


## Write output to a fasta file that can be blasted
transpose table
```{r, eval=TRUE}
seqtab.nochim_trans <- as.data.frame(t(seqtab.nochim)) %>% rownames_to_column(var = "sequence") %>% 
    rowid_to_column(var = "OTUNumber") %>% mutate(OTUNumber = sprintf("otu%04d", 
    OTUNumber)) %>% mutate(sequence = str_replace_all(sequence, "(-|\\.)", ""))
```

convert to fasta file
```{r, eval=TRUE}
df <- seqtab.nochim_trans
seq_out <- Biostrings::DNAStringSet(df$sequence)

names(seq_out) <- str_c(df$OTUNumber, df$Supergroup, df$Division, df$Class, 
    df$Order, df$Family, df$Genus, df$Species, sep = "|")

Biostrings::writeXStringSet(seq_out, str_c( "Zoop_ASV.fasta"), compress = FALSE, 
    width = 20000)

#I had to move this file from my working directory to my data directory
```

examine fasta file
```{r, engine="bash", eval=TRUE}
head /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/data/Zoop_ASV.fasta
```

## Run Blast

Since we are looking at nucleotides, make sure you are using blastn.
```{r, engine="bash"}
/Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/software/ncbi-blast-2.14.0+/bin/blastn \
-query /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/data/Zoop_ASV.fasta \
-db /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/blastdb/MZGfasta-coi__MZGdbALL__o00__A \
-out /Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/output/Zoop_ASV.tab \
-evalue 1E-20 \
-num_threads 8 \
-max_target_seqs 1 \
-outfmt 6
```

Examine blast output
```{r, engine="bash", eval=TRUE}
head -2 ../output/Zoop_ASV.tab
wc -l ../output/Zoop_ASV.tab
```
These are species we expect to see - looks like blast worked!


# Visualization


## Filter Blast hits and merge tables

merge Blast IDs with OTU table

```{r, eval=TRUE}
#change directory
setwd("/Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/output")

#read ASV table into R
Zoop_ASV<-read.table("Zoop_ASV.tab")

#rename columns
colnames(Zoop_ASV) = c("OTUNumber", "Species", "pident", "length","mismatch","gapopen","qstart","qend","sstart","send","evalue","bitscore")

#merge tables by otu number
read_table<-left_join(Zoop_ASV, seqtab.nochim_trans, by = join_by("OTUNumber" == "OTUNumber"))
datatable(read_table)
```
Visualize part of the table here


Filter hits
```{r, eval=TRUE}
# remove ASV with sequences shorter than 300 bp
read_table_sub <- subset(read_table, read_table$length>300) 

#remove ASV with pident <95%
read_table_sub2 <- subset(read_table_sub, read_table_sub$pident>95) 

```

Sum Sequences by Taxa
```{r, eval=TRUE}
#remove unneeded columns
read_table_summed<- read_table_sub2[ -c(1,3:13) ]

#remove prefix
read_table_summed$Species <- sub(".*__", "", read_table_summed$Species)

#summarize by species
by_species <- read_table_summed %>%
  group_by(Species)

read_table_species<-by_species %>%
  summarise_all(sum)

datatable(read_table_species)

```


## NMDS
```{r, eval=TRUE}
# Transpose data
read_table_transposed <- t(read_table_species)
datatable(read_table_transposed)
```

Prep table for NMDS
```{r, eval=TRUE}
# get row and colnames in order
rownames(read_table_transposed) <- colnames(read_table_species)
#move first row to column names
read_table_final<-row_to_names(read_table_transposed, 1, remove_rows_above = FALSE)
```

```{r, eval=TRUE}
#convert to matrix
community_matrix<-as.matrix(read_table_final)
#convert to numeric matrix
community_matrix<- matrix(as.numeric(community_matrix),   
                  ncol = ncol(community_matrix))
#convert to proportions
community_matrix<-community_matrix/rowSums(community_matrix)
#arcsine sqrt transformation
community_matrix_sqrt<-asin(sqrt(community_matrix))
```

Run NMDS
```{r, eval=TRUE}
#run NMDS
NMDS=metaMDS(community_matrix_sqrt,distance="bray",trymax=100)
```

```{r, eval=TRUE}
stressplot(NMDS)
plot(NMDS)
NMDS
```
Make envrionmental table
```{r, eval=TRUE}
#export file names
env <- as.data.frame(row.names(read_table_final))
#change column name
colnames(env)[colnames(env) == "row.names(read_table_final)"] <- "file"

#create year column
env$year <- substr(env$file, 1, 4)
#create month column
env$month <- substr(env$file, 5, 7)
#create station column
env$station <- str_extract(env$file, "P12|P22|P28|P38|P402|P4|P8")
#create run column
env$run <- str_extract(env$file, "48samples|-P1|-P2")
```

extract scores
```{r, eval=TRUE}
data.scores = as.data.frame(scores(NMDS)$sites)
#add environmental columns
data.scores$station = env$station
data.scores$year = env$year
data.scores$month = env$month
data.scores$run = env$run
```

create station plot
```{r, eval=TRUE}
station_plot = ggplot(data = data.scores, aes(x = NMDS1, y = NMDS2)) + 
     geom_point(data = data.scores, aes(colour = station), size = 3, alpha = 0.5) + 
     scale_colour_manual(values = c("orange", "steelblue","darkgreen","violet","red","darkblue","limegreen")) + 
     theme(axis.title = element_text(size = 10, face = "bold", colour = "grey30"), 
     panel.background = element_blank(), panel.border = element_rect(fill = NA, colour = "grey30"), 
     axis.ticks = element_blank(), axis.text = element_blank(), legend.key = element_blank(), 
     legend.title = element_text(size = 10, face = "bold", colour = "grey30"), 
     legend.text = element_text(size = 9, colour = "grey30")) +
     labs(colour = "Station")
     
station_plot

```


There is quite a bit of overlap among stations, but you can see that some stations are distinct from one another. For example, P22 is different from P38, but P38 and P12 are very similar.

create year plot

```{r, eval=TRUE}
year_plot = ggplot(data = data.scores, aes(x = NMDS1, y = NMDS2)) + 
     geom_point(data = data.scores, aes(colour = year), size = 3, alpha = 0.5) + 
     scale_colour_manual(values = c("orange", "steelblue")) + 
     theme(axis.title = element_text(size = 10, face = "bold", colour = "grey30"), 
     panel.background = element_blank(), panel.border = element_rect(fill = NA, colour = "grey30"), 
     axis.ticks = element_blank(), axis.text = element_blank(), legend.key = element_blank(), 
     legend.title = element_text(size = 10, face = "bold", colour = "grey30"), 
     legend.text = element_text(size = 9, colour = "grey30")) +
     labs(colour = "Year")
     
year_plot

```

between these two years, there doesn't seem to be much difference

create month plot
```{r, eval=TRUE}
month_plot = ggplot(data = data.scores, aes(x = NMDS1, y = NMDS2)) + 
     geom_point(data = data.scores, aes(colour = month), size = 3, alpha = 0.5) + 
     scale_colour_manual(values = c("orange", "steelblue","darkgreen")) + 
     theme(axis.title = element_text(size = 10, face = "bold", colour = "grey30"), 
     panel.background = element_blank(), panel.border = element_rect(fill = NA, colour = "grey30"), 
     axis.ticks = element_blank(), axis.text = element_blank(), legend.key = element_blank(), 
     legend.title = element_text(size = 10, face = "bold", colour = "grey30"), 
     legend.text = element_text(size = 9, colour = "grey30")) +
     labs(colour = "Month")
     
month_plot

```

There seem to be some clear seasonal differences along axis 1 - April appears to be distinct

create run plot
```{r, eval=TRUE}
run_plot = ggplot(data = data.scores, aes(x = NMDS1, y = NMDS2)) + 
     geom_point(data = data.scores, aes(colour = run), size = 3, alpha = 0.5) + 
     scale_colour_manual(values = c("orange", "steelblue","darkgreen")) + 
     theme(axis.title = element_text(size = 10, face = "bold", colour = "grey30"), 
     panel.background = element_blank(), panel.border = element_rect(fill = NA, colour = "grey30"), 
     axis.ticks = element_blank(), axis.text = element_blank(), legend.key = element_blank(), 
     legend.title = element_text(size = 10, face = "bold", colour = "grey30"), 
     legend.text = element_text(size = 9, colour = "grey30")) +
     labs(colour = "Run")
     
run_plot

```


## Diversity Indices

```{r, eval=TRUE}
#convert to matrix
diversity<-as.matrix(read_table_final)
#convert to numeric matrix
diversity_matrix<- matrix(as.numeric(diversity),   
                  ncol = ncol(diversity))
#add in row and column labels
colnames(diversity_matrix) <- colnames(diversity)
rownames(diversity_matrix) <- rownames(diversity)
```

get shannon diversity index
```{r, eval=TRUE}
shannon_scores<-as.data.frame(diversity(diversity_matrix, index="shannon"))
```

add environment columns back in
```{r, eval=TRUE}
#change column name
colnames(shannon_scores)[1] <- "shannon_index"
#add environmental columns
shannon_scores$station = env$station
shannon_scores$year = env$year
shannon_scores$month = env$month
shannon_scores$run = env$run
```

make violin plot
```{r, eval=TRUE}
diversity<- ggviolin(shannon_scores, x = "station", y = "shannon_index",
 add = "boxplot", fill = "station") 
diversity
```

Overall, the highest diversity was seen at P22, which is the station located in the Strait of Juan de Fuca. This site is closest to the ocean, so it may contain inland taxa as well as more offshore taxa.


## Taxa plot

get the top 20 most abundant species
```{r, eval=TRUE}
#convert to proportions
diversity_prop<-diversity_matrix/rowSums(diversity_matrix)
species_means <- as.data.frame(colMeans(diversity_prop))
colnames(species_means)[1] <- "abundance"
species_means$abundance<-as.numeric(species_means$abundance)
species_means <- rownames_to_column(species_means, "species")
species_means <-as.data.frame(species_means[order(-species_means$abundance),])

#get top 20 species
top_20_species <- head(species_means, 20)
```

plot
```{r, eval=TRUE}
ggplot(top_20_species, aes(y=abundance, x=reorder(species, abundance))) + 
    geom_bar(position="dodge", stat="identity")+ theme_bw()+coord_flip()+ylab("Mean Proportion of Reads")+xlab("species")
```

The samples were dominated by Calanus pacificus (a copepod) and Aegina citrea (a jellyfish). Other abundant species included eutonina indicans and Clytia gregaria, two other jellyfish. Other common copepods like Centropages abdonminalis and Metridia pacifica were also represented

## Plot Pie Charts on a Map

Reformat data
```{r}
# convert format
diversity_melt<-melt(diversity_matrix)
#get list of top 20 species
top_20<-as.list(top_20_species$species)
#subset dataframe to only include top 20
diversity_melt_top <- as.data.frame(diversity_melt[diversity_melt$Var2 %in% top_20,])
#get proportions
diversity_melt_prop<-diversity_melt_top  %>% 
  mutate(freq = prop.table(value), .by = Var1)
```
Add latitude and longitude to data
```{r}
#read in latlong table
latlong <- "/Users/hailaschultz/GitHub/haila-ZoopMetabarcoding/data/latlong.csv" 
lat_long <- read.csv(latlong)
#add latlong to env table
env_lat_lon<-left_join(lat_long, env, by = join_by(Station == station))
#add new environment table to data
map_data<-left_join(env_lat_lon,diversity_melt_prop, by = join_by(file == Var1))
#subset to April 2018
map_data_sub<-map_data[map_data$month == 'APR', ]
map_data_sub<-map_data_sub[map_data_sub$year == '2018', ]
map_data_sub<-map_data_sub[map_data_sub$run == '48samples', ]
```


get google map
```{r, eval=TRUE}
library(RgoogleMaps)

terrmap <- GetMap(zoom=9, size = c(700,
900),center=c(lat=47.8,lon=-122.6),maptype= "satellite", destfile = "terrain.png") #lots of visual options, just like google maps: maptype = c("roadmap", "mobile", "satellite", "terrain", "hybrid", "mapmaker-roadmap", "mapmaker-hybrid")
Puget_map<-PlotOnStaticMap(terrmap)
```

add pies
```{r}
library(mapplots)
#convert x and y
xy <- LatLon2XY.centered(terrmap,map_data_sub$Lat.deg, map_data_sub$Lon.deg, zoom = 9)
map_data_sub$newX=xy$newX
map_data_sub$newY=xy$newY
#dcast
map_data_sub2<-dcast(map_data_sub, Station+newX+newY ~ Var2,value.var="freq")
make.xyz <- function(x, y, z, category) {
  # Assuming x, y, z, and category are vectors of equal length
  # Create a list with x, y, and z components
  xyz <- list(ap_data_sub$newX, map_data_sub$newY, z = map_data_sub$freq, map_data_sub$Var2)
  return(xyz)
}
#convert 0 to 0.001
map_data_sub2[, 4:23] <- abs(map_data_sub2[, 4:23]) + 0.001
# Loop over each row in the dataframe
for (i in 1:nrow(map_data_sub)) {
  # Get the current location, categories, and values
  xyz <- make.xyz(map_data_sub$newX, map_data_sub$newY, z = map_data_sub$freq, map_data_sub$Var2)[[i]]
  # Draw the pie chart for the current location
  add.pie(xyz$z, xyz$x, xyz$y)
}


```


```{r}
xyz
```






